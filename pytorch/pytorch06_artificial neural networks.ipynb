{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ad5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "#import ipywidgets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection    \n",
    " \n",
    "# use LaTeX, choose nice some looking fonts and tweak some settings\n",
    "matplotlib.rc('font', family='serif')\n",
    "matplotlib.rc('font', size=16)\n",
    "matplotlib.rc('legend', fontsize=16)\n",
    "matplotlib.rc('legend', numpoints=1)\n",
    "matplotlib.rc('legend', handlelength=1.5)\n",
    "matplotlib.rc('legend', frameon=True)\n",
    "matplotlib.rc('xtick.major', pad=7)\n",
    "matplotlib.rc('xtick', direction=\"in\")\n",
    "matplotlib.rc('ytick', direction=\"in\")\n",
    "matplotlib.rc('xtick', top = True)\n",
    "matplotlib.rc('ytick', right =True )\n",
    "matplotlib.rc('xtick.minor', pad=7)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "# matplotlib.rc('text.latex', \n",
    "#               preamble=[r'\\usepackage[T1]{fontenc}',\n",
    "#                         r'\\usepackage{amsmath}',\n",
    "#                         r'\\usepackage{txfonts}',\n",
    "#                         r'\\usepackage{textcomp}'])\n",
    "\n",
    "matplotlib.rc('figure', figsize=(12, 9))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d36c9",
   "metadata": {},
   "source": [
    "## 머신 러닝 모델의 평가.\n",
    "\n",
    "- 실제 모델을 평가하기 위해서 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하는 것이 일반적임. 다만 이 책의 목적은 개념 학습이므로 일부 실습에서는 별도로 세 가지로 분리하지 않고 훈련용, 테스트용으로만 분리해서 사용힘.\n",
    "\n",
    "- 검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도. \n",
    "\n",
    "- **하이퍼파라미터(초매개변수)** 란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말합니다. 반면, 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 이 책에서는 **매개변수**라고 부른다.\n",
    "\n",
    "- 하이퍼파라미터와 매개변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점(선형 회귀 챕터에서 배우게 되는 경사 하강법에서 학습률(learning rate), 딥 러닝에서는 은닉층의 수, 뉴런의 수, 드롭아웃 비율 등) \n",
    "- 반면 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값. \n",
    "\n",
    "- 훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning)한다. 또한 이 모델의 매개변수는 검증용 데이터로 정확도가 검증되는 과정에서 점차 검증용 데이터에 점점 맞추어져 가기 시작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea498f6",
   "metadata": {},
   "source": [
    "## 혼동 행렬(Confusion Matrix)\n",
    "\n",
    "- 머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 한다. 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않으므로 **혼동 행렬(Confusion Matrix)** 을 사용한다.\n",
    "\n",
    "- 예를 들어 양성(Positive)과 음성(Negative)을 구분하는 이진 분류가 있다고 하였을 때 혼동 행렬은 다음과 같습니다. 각 열은 예측값을 나타내며, 각 행은 실제값을 나타냅니다.\n",
    "\n",
    "|-\t|참\t|거짓|\n",
    "|---|:---:|:---:|\n",
    "|참\t|TP\t|FN|\n",
    "|거짓\t|FP|\tTN|\n",
    "\n",
    "- 이를 각각 TP(True Positive), TN(True Negative), FP(False Postivie), FN(False Negative)라고 하는데 True는 정답을 맞춘 경우고 False는 정답을 맞추지 못한 경우입니다. 그리고 Positive와 Negative는 각각 제시했던 정답입니다. 즉, TP는 양성(Postive)이라고 대답하였고 실제로 양성이라서 정답을 맞춘 경우입니다. TN은 음성(Negative)이라고 대답하였는데 실제로 음성이라서 정답을 맞춘 경우입니다.\n",
    "\n",
    "- 그럼 FP는 양성이라고 대답하였는데, 음성이라서 정답을 틀린 경우이며 FN은 음성이라고 대답하였는데 양성이라서 정답을 틀린 경우가 됩니다. 그리고 이 개념을 사용하면 또 새로운 개념인 정밀도(Precision)과 재현률(Recall)이 됩니다.\n",
    "\n",
    "- 정밀도(Precision)\n",
    "정밀도은 양성이라고 대답한 전체 케이스에 대한 TP의 비율입니다. 즉, 정밀도를 수식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "정밀도=\\dfrac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "2) 재현률(Recall)\n",
    "재현률은 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율입니다. 즉, 양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지를 나타냅니다.\n",
    "\n",
    "$$\n",
    "재현률=\\dfrac{TP}{TP+FN}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d89448",
   "metadata": {},
   "source": [
    "## 퍼셉트론\n",
    "- 퍼셉트론(Perceptron)은 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘.\n",
    "\n",
    "<img src=\"./perceptron.png\" width=\"20%\">\n",
    "\n",
    "- $x=(x_1,\\ldots,\\,x_n)$ 는 입력값을 의미하며, $W=(W_1,\\ldots,\\,W_n)$ 는 가중치(Weight), $y$ 는 출력값, 그림 안의 원은 인공 뉴런.\n",
    "\n",
    "- 각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 그에 해당되는 가중치의 곱의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로서 1을 출력하고, 그렇지 않을 경우에는 0을 출력함. 이러한 함수를 계단 함수(Step function)라고 한다. \n",
    "\n",
    "- 이때 계단 함수에 사용된 이 임계치값을 수식으로 표현할 때는 보통 세타($\\theta$)로 표현한다.\n",
    "$$\n",
    "y = 1 \\text{ if } \\sum_{i=1}^n W_i x_i \\ge \\theta \\,,\\\\\n",
    "y = 0 \\text{ if } \\sum_{i=1}^n W_i x_i > \\theta \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6dc24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6275a06",
   "metadata": {},
   "source": [
    "## Pytorch 로 단층 퍼셉트론 구현하기\n",
    "\n",
    "#### 1개의 뉴런을 가지는 단층 퍼셉트론 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bca887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7273974418640137\n",
      "100 0.6931475400924683\n",
      "200 0.6931471824645996\n",
      "300 0.6931471824645996\n",
      "400 0.6931471824645996\n",
      "500 0.6931471824645996\n",
      "600 0.6931471824645996\n",
      "700 0.6931471824645996\n",
      "800 0.6931471824645996\n",
      "900 0.6931471824645996\n",
      "1000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# 이제 XOR 문제에 해당되는 입력과 출력을 정의합니다.\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# 이제 1개의 뉴런을 가지는 단층 퍼셉트론을 구현해봅시다. 단층 퍼셉트론이 처음 소개되었을 때는 계단 함수였지만, 우리는 이미 또 다른 활성화 함수인 시그모이드 함수를 알고 있으므로 시그모이드 함수를 사용해보겠습니다.\n",
    "linear = nn.Linear(2, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "# 비용 함수와 옵티마이저 정의\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "#10,001번의 에포크 수행. 0번 에포크부터 1,000번 에포크까지.\n",
    "for step in range(1001): \n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0: # 100번째 에포크마다 비용 출력\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df08409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "모델의 예측값(Predicted):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "실제값(Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  0.5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ab75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfae9df0",
   "metadata": {},
   "source": [
    "## 인공신경망에서의 순전파와 역전파\n",
    "\n",
    "#### 1. 인공 신경망의 이해(Neural Network Overview)\n",
    "\n",
    "- 인공 신경망의 예재를 소개합니다. 역전파의 이해를 위해서 여기서 사용할 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가집니다. 또한 해당 인공 신경망은 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용합니다. 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용합니다.\n",
    "\n",
    "<img src=\"sample_nn_1.png\" width=\"25%\">\n",
    "\n",
    "위의 그림은 여기서 사용할 인공 신경망의 모습을 보여줍니다. \n",
    "\n",
    "- 은닉층과 출력층의 모든 뉴런에서 변수 $z$가 존재하는데 여기서 변수 $z$는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미한다. 이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태이다. 즉, 활성화 함수의 입력을 의미한다. \n",
    "- $z$ 우측의 $\\mid$ 를 지나서 존재하는 변수 $h$ 또는 $o$ 는 $z$가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미합니다. \n",
    "- 이번 역전파 예제에서는 인공 신경망에 존재하는 모든 가중치 $W$에 대해서 역전파를 통해 업데이트하는 것을 목표로합니다. 일단 해당 인공 신경망은 편향 $b$ 는 고려하지 않습니다.\n",
    "\n",
    "#### 2. 순전파(Forward Propagation)\n",
    "\n",
    "<img src=\"./sample_forwardpropagation_1.png\" width=\"30%\">\n",
    "\n",
    "- 주어진 값이 위의 그림과 같을 때 순전파를 진행해봅시다. 파란색 숫자는 입력값을 의미하며, 빨간색 숫자는 각 가중치의 값을 의미합니다. 앞으로 진행하는 계산의 결과값은 소수점 아래 여덟번째 자리까지 반올림하여 표기합니다.\n",
    "\n",
    "- 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됩니다. $z_1$ 과 $z_2$ 는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당됩니다.\n",
    "\n",
    "$$\n",
    "z_1=W_1x_1+W_2x_2=0.3 \\times 0.1 + 0.25\\times 0.2 = 0.08\\,,\\\\\n",
    "z_2=W_3x_1+W_4x_2=0.4 \\times 0.1 + 0.35 \\times 0.2=0.11\\,.\n",
    "$$\n",
    "\n",
    "- $z_1$ 과 $z_2$ 는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 되는데 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값입니다. 식에서는 각각 $h_1$ 과 $h_2$ 에 해당되며, 아래의 결과와 같습니다.\n",
    "\n",
    "$$\n",
    "h_1 = \\operatorname{sigmoid}(z_1) = 0.51998934 \\,,\\\\\n",
    "h_2 = \\operatorname{sigmoid}(z_2)=0.52747230\\,.\n",
    "$$\n",
    "\n",
    "- $h_1$ 과 $h_2$ 이 두 값은 다시 출력층의 뉴런으로 향하게 되는데 이때 다시 각각의 값에 해당되는 가중치와 곱해지고, 다시 가중합 되어 출력층 뉴런의 시그모이드 함수의 입력값이 됩니다. 식에서는 각각 $z_3$ 과 $z_4$에 해당됩니다.\n",
    "\n",
    "$$\n",
    "z_3= W_ 5 h_1 + W_6 h_2 = 0.45 \\times h_1+0.4 \\times h_2=0.44498412 \\,,\\\\\n",
    "z_4= W_7 h_1 + W_8 h_ 2 = 0.7 \\times h_1 + 0.6 \\times h_2=0.68047592 \\,.\n",
    "$$\n",
    "\n",
    "- $z_3$ 과 $z_4$ 이 출력층 뉴런에서 시그모이드 함수를 지난 값은 이 인공 신경망이 최종적으로 계산한 출력값입니다. 실제값을 예측하기 위한 값으로서 예측값이라고도 부릅니다.\n",
    "\n",
    "$$\n",
    "o_1 = \\operatorname{sigmoid}(z_3) = 0.60944600 \\,,\\\\\n",
    "o_2 = \\operatorname{sigmoid}(z_4) = 0.66384491 \\,.\n",
    "$$\n",
    "\n",
    "- 이제 해야할 일은 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택하는 것입니다. 오차(Error)를 계산하기 위한 손실 함수(Loss function)로는 평균 제곱 오차 MSE를 사용합니다. 식에서는 실제값을 $\\operatorname{target}$이라고 표현하였으며, 순전파를 통해 나온 예측값을 $\\operatorname{output}$으로 표현하였습니다. 그리고 각 오차를 모두 더하면 전체 오차 $E_{total}$가 됩니다.\n",
    "\n",
    "$$\n",
    "E_{o1}=\\dfrac{1}{2}(\\text{target}_{o1}−\\text{output}_{o1})^2=0.02193381 \\,,\\\\\n",
    "E_{o2}=\\dfrac{1}{2}(\\text{target}_{o2}−\\text{output}_{o2})^2=0.00203809\\,,\\\\\n",
    "E_{total}=E_{o1}+E_{o2}=0.02397190\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358af6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c372a99",
   "metadata": {},
   "source": [
    "#### 3. 역전파 1단계(Back Propagation Step 1)\n",
    "\n",
    "<img src = \"./sample_backwardpropagation_1.png\" width=\"40%\">\n",
    "\n",
    "- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해갑니다. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 $N$층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 $N$층과 $N$층의 이전층 사이의 가중치를 업데이트 하는 단계를 역전파 2단계라고 해봅시다.\n",
    "\n",
    "\n",
    "- 역전파 1단계에서 업데이트 해야 할 가중치는 $W_5,\\,W_6,\\,W_7,\\,W_8$ 로 총 4개입니다. 원리 자체는 동일하므로 우선 $W_5$에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 $W_5$를 업데이트 하기 위해서 $\\dfrac{\\partial E_{total}}{\\partial W_5}$를 계산해야 합니다.\n",
    "\n",
    "- $\\dfrac{\\partial E_{total}}{\\partial W_5}$ 를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_5} = \\dfrac{\\partial E_{total}}{\\partial o_1} \\times \\dfrac{\\partial o_1}{\\partial z_3} \\times \\dfrac{\\partial z_3}{\\partial W_5}\n",
    "$$\n",
    "\n",
    "- 위의 식에서 우변의 세 개의 각 항에 대해서 순서대로 계산해봅시다. 우선 첫번째 항에 대해서 계산해보겠습니다. 미분을 진행하기 전에 $E_{total}$의 값을 상기해봅시다. $E_{total}$ 은 앞서 순전파를 진행하고 계산했던 전체 오차값입니다. 식은 다음과 같습니다.\n",
    "\n",
    "\n",
    "$$\n",
    "E_{total}=\\dfrac{1}{2}\\left( \\operatorname{target}_{o1} - \\operatorname{output}_{o1} \\right)^2  + \\dfrac{1}{2}\\left( \\operatorname{target}_{o2} -\\operatorname{output}_{o2} \\right)^2\n",
    "$$\n",
    "\n",
    "이에 $\\dfrac{\\partial E_{total}}{\\partial o_1}$ 는 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial o_1} = 2 \\times \\dfrac{1}{2} \\left( \\operatorname{target}_{o1} - \\operatorname{output}_{o1} \\right)^{2 -1} \\times (-1) + 0\\,,\\\\\n",
    "\\dfrac{\\partial E_{total}}{\\partial o_1} = - \\left( \\operatorname{target}_{o1}- \\operatorname{output}_{o1} \\right) = -(0.4 - 0.60944600) = 0.20944600\\,.\n",
    "$$\n",
    "\n",
    "- 이제 두번째 항을 주목해봅시다. $o_1$ 이라는 값은 시그모이드 함수의 출력값입니다. 그런데 시그모이드 함수 $f(x)=\\dfrac{1}{1+e^{-x}}$ 의 미분은 $f'(x) = f(x) \\times(1 -f(x))$ 입니다. 앞으로의 계산 과정에서도 계속해서 시그모이드 함수를 미분해야 하는 상황이 생기므로 기억해둡시다. 이에 따라서 두번째 항의 미분 결과는 다음과 같습니다. (시그모이드 함수 미분 참고 링크 : https://en.wikipedia.org/wiki/Logistic_function#Derivative)\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial o_1}{\\partial z_3} = o_1 \\times (1 - o_1) = 0.60944600\\times (1 - 0.60944600)=0.23802157\n",
    "$$\n",
    "\n",
    "- 마지막으로 세번째 항은 $h_1$ 의 값과 동일합니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial z_3}{\\partial W_5}= h_ 1=0.51998934\\;.\n",
    "$$\n",
    "\n",
    "- 우변의 모든 항을 계산하였습니다. 이제 이 값을 모두 곱해주면 됩니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{W_5} = 0.20944600 \\times 0.23802157 \\times 0.51998934 = 0.02592286\\;.\n",
    "$$\n",
    "\n",
    "- 이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 때가 왔습니다! 하이퍼파라미터에 해당되는 학습률(learning rate) $\\alpha$ 는 $0.5$ 라고 가정합니다.\n",
    "\n",
    "$$\n",
    "W^+_5=W_5−\\alpha \\dfrac{\\partial E_{total}}{\\partial W_5} = 0.45 - 0.5 \\times 0.02592286 = 0.43703857\\;.\n",
    "$$\n",
    "\n",
    "- 이와 같은 원리로 $W^+_6,\\, W^+_7,\\, W^+_8$을 계산할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_6}=\\dfrac{\\partial E_{total}}{\\partial o_1} \\times \\dfrac{\\partial o_1}{\\partial z_3} \\times  \\dfrac{\\partial z_3}{\\partial W_6} \\longrightarrow W^+_6 = 0.38685205 \\,,\\\\\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_7} = \\dfrac{\\partial E_{total}}{\\partial o_2}\\times \\dfrac{\\partial o_2 }{\\partial z_4} \\times \\dfrac{\\partial z_4}{\\partial W_7} \\longrightarrow W^+_7=0.69629578 \\,,\\\\\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_8} = \\dfrac{\\partial E_{total}}{\\partial o_2} \\times  \\dfrac{\\partial o_2}{\\partial z_4} \\times  \\dfrac{\\partial z_4}{\\partial W_8} \\longrightarrow W^+_8 = 0.59624247\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e7ec4",
   "metadata": {},
   "source": [
    "#### 4. 역전파 2단계 (Back Propagation Step 2)\n",
    "\n",
    "<img src = \"./sample_backwardpropagation_2.png\" width=\"40%\">\n",
    "\n",
    "- 1단계를 완료하였다면 이제 입력층 방향으로 이동하며 다시 계산을 이어갑니다. 위의 그림에서 빨간색 화살표는 순전파의 정반대 방향인 역전파의 방향을 보여줍니다. 현재 인공 신경망은 은닉층이 1개밖에 없으므로 이번 단계가 마지막 단계입니다. 하지만 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속해서 계산해가야 합니다.\n",
    "\n",
    "- 이번 단계에서 계산할 가중치는 $W_1,\\,W_2,\\,W_3,\\,W_4$입니다. 원리 자체는 동일하므로 우선 $W_1$에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 $W_1$를 업데이트 하기 위해서 $\\dfrac{\\partial E_{total}}{\\partial W_1}$를 계산해야 합니다.\n",
    "\n",
    "- $\\dfrac{\\partial E_{total}}{\\partial W_1}$ 를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_1} = \\dfrac{\\partial E_{total}}{\\partial h_1} \\times  \\dfrac{\\partial h_1}{\\partial z_1} \\times  \\dfrac{\\partial z_1}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "- 위의 식에서 우변의 첫번째항인 ∂Etotal∂h1는 다음과 같이 다시 식을 풀어서 쓸 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial h_1} = \\dfrac{\\partial E_{o_1}}{\\partial h_1}+\\dfrac{\\partial E_{o2}}{\\partial h_1}\n",
    "$$\n",
    "\n",
    "- 위의 식의 우변의 두 항을 각각 구해봅시다. 우선 첫번째 항 $\\dfrac{\\partial E_{o1}}{\\partial h_1}$에 대해서 항을 분해 및 계산해보겠습니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial E_{o1}}{\\partial h_1}&=\\dfrac{\\partial E_{o1}}{z_3} \\times \\dfrac{\\partial z_3}{\\partial h_1} = \\dfrac{\\partial E_{o1}}{\\partial o_1} \\times \\dfrac{\\partial o_1}{\\partial z_3} \\times \\dfrac{\\partial z_3}{\\partial h_1} \\\\\n",
    "&= -(\\operatorname{target}_{o1}−\\operatorname{output}_{o1}) \\times o_1 \\times (1 - o_1) \\times W_5 \\\\\n",
    "&= 0.20944600 \\times 0.23802157 \\times0.45\\\\\n",
    "&=0.02243370\\;.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 이와 같은 원리로 $\\dfrac{\\partial E_{o2}}{ \\partial h_1}$ 또한 구합니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial E_{o2}}{\\partial h_1} &= \\dfrac{\\partial E_{o2}}{\\partial z_4} \\times \\dfrac{ \\partial z_4}{\\partial h_1} = \\dfrac{\\partial E_{o2}}{\\partial o_2} \\times  \\dfrac{\\partial o_2}{\\partial z_4}\\times \\dfrac{\\partial z_4}{\\partial h_1} = 0.00997311 \\,,\\\\\n",
    "\\dfrac{ \\partial E_{total}}{\\partial h_1}&=0.02243370+0.00997311=0.03240681\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 이제 $\\dfrac{\\partial E_{total}}{\\partial W_1}$ 를 구하기 위해서 필요한 첫번째 항을 구했습니다. 나머지 두 항에 대해서 구해보도록 하겠습니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial h_1}{\\partial z_1} &= h_1 \\times (1 - h_1) = 0.51998934(1-0.51998934)=0.24960043 \\,,\\\\\n",
    "\\dfrac{\\partial z_1}{\\partial W_1} &= x_1 = 0.1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 즉, $\\dfrac{\\partial E_{total}}{\\partial W_1}$ 는 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_1}=0.03240681 \\times 0.24960043 \\times 0.1 = 0.00080888\n",
    "$$\n",
    "\n",
    "- 이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 수 있습니다.\n",
    "\n",
    "\n",
    "$$\n",
    "W^+_1=W_1−\\alpha \\dfrac{\\partial E_{total}}{\\partial W_1} = 0.1 - 0.5\\times 0.00080888 = 0.29959556\n",
    "$$\n",
    "\n",
    "이와 같은 원리로 $W^+_2,\\, W^+_3,\\, W^+_4$ 을 계산할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_2} &= \\dfrac{\\partial E_{total}}{\\partial h_1} \\times  \\dfrac{\\partial h_1}{\\partial z_1} \\times \\dfrac{\\partial z_1}{\\partial W_2} \\longrightarrow W^+_2 = 0.24919112\\,,\\\\\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_3} &= \\dfrac{\\partial E_{total}}{\\partial h_2}\\times \\dfrac{\\partial h_2} {\\partial z_2}\\times \\dfrac{\\partial z_2}{\\partial W_3}\\longrightarrow W^+_3 = 0.39964496\\,,\\\\\n",
    "\\dfrac{\\partial E_{total}}{\\partial W_4} &= \\dfrac{\\partial E_{total}}{\\partial h_2} \\times  \\dfrac{\\partial h_2}{\\partial z_2} \\times \\dfrac{\\partial z_2}{\\partial W_4} \\longrightarrow W^+_4 = 0.34928991\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<u>인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말합니다.</u>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de563d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cafb7c46",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론으로 XOR 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6139f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 0.00013882899656891823\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# XOR 문제를 풀기 위한 입력과 출력을 정의해줍니다.\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# 이제 다층 퍼셉트론을 설계합니다. 아래는 입력층, 은닉층1, 은닉층2, 은닉층3, 출력층을 가지는 은닉층이 3개인 인공 신경망입니다.\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Sigmoid()\n",
    "          ).to(device)\n",
    "\n",
    "# 비용 함수와 옵타마이저를 선언합니다. nn.BCELoss()는 이진 분류에서 사용하는 크로스엔트로피 함수입니다.\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
    "\n",
    "# 총 10,001번의 에포크를 수행합니다. 각 에포크마다 역전파가 수행된다고 보면 되겠습니다.\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    # forward 연산\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100의 배수에 해당되는 에포크마다 비용을 출력\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(epoch, cost.item())\n",
    "print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ab5148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[1.8572781e-04]\n",
      " [9.9975139e-01]\n",
      " [9.9973148e-01]\n",
      " [3.5759291e-04]]\n",
      "모델의 예측값(Predicted):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "실제값(Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4007fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1cecb5",
   "metadata": {},
   "source": [
    "## 비선형 활성화 함수(Activation function)\n",
    "\n",
    "#### 1. 활성화 함수의 특징 - 비선형 함수(Nonlinear function)\n",
    "- 활성화 함수의 특징은 선형 함수가 아닌 비선형 함수여야 한다는 점입니다. 선형 함수란 출력이 입력의 상수배만큼 변하는 함수를 선형함수라고 합니다. 예를 들어 $f(x)=Wx+b$ 라는 함수가 있을 때, $W$와 $b$는 상수입니다. 이 식은 그래프를 그리면 직선이 그려집니다. 반대로 비선형 함수는 직선 1개로는 그릴 수 없는 함수를 말합니다.\n",
    "\n",
    "- 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야 합니다. 그런데 만약 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없습니다. 예를 들어 활성화 함수로 선형 함수를 선택하고, 층을 계속 쌓는다고 가정해보겠습니다. 활성화 함수는 $f(x)=Wx$ 라고 가정합니다. 여기다가 은닉층을 두 개 추가한다고하면 출력층을 포함해서 $y(x)=f(f(f(x)))$ 가 됩니다. 이를 식으로 표현하면 $W \\times W\\times W \\times x$ 입니다. 그런데 이는 잘 생각해보면 $W$ 의 세 제곱값을 $k$ 라고 정의해버리면 $y(x)=kx$ 와 같이 다시 표현이 가능합니다. 즉, 선형 함수로는 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이를 줄 수 없습니다.\n",
    "\n",
    "- 선형 함수를 사용한 은닉층을 1회 추가한 것과 연속으로 추가한 것이 차이가 없다는 뜻이지, 선형 함수를 사용한 층이 아무 의미가 없다는 뜻이 아닙니다. 학습 가능한 가중치가 새로 생긴다는 점에서 분명히 의미가 있습니다. 이와 같이 선형 함수를 사용한 층을 활성화 함수를 사용하는 은닉층과 구분하기 위해서 선형층(linear layer)이나 투사층(projection layer) 등의 다른 표현을 사용하여 표현하기도 합니다. 활성화 함수를 사용하는 일반적인 은닉층을 선형층과 대비되는 표현을 사용하면 비선형층(nonlinear layer)입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3174f08",
   "metadata": {},
   "source": [
    "#### 2. 시그모이드 함수(Sigmoid function)와 기울기 소실\n",
    "시그모이드 함수를 사용한 어떤 인공 신경망이 있다고 가정해보겠습니다.\n",
    "\n",
    "<img src = \"./simple-neural-network.png\" width=\"40%\">\n",
    "\n",
    "- 위 인공 신경망의 학습 과정은 다음과 같습니다. 우선 인공 신경망은 입력에 대해서 순전파(forward propagation) 연산을 하고, 그리고 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산하고, 그리고 이 손실(loss)을 미분을 통해서 기울기(gradient)를 구하고, 이를 통해 역전파(back propagation)를 수행합니다.\n",
    "\n",
    "- 시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4fcfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efd7d5d39d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "x1=np.arange(-10.0, 10.0, 0.01)\n",
    "fig=plt.figure(figsize=(8, 6))\n",
    "plt.plot(x1, sigmoid(x1), label=\"Sigmoid($x$)\")\n",
    "plt.plot(x1, np.tanh(x1), label=\"tanh($x$)\")\n",
    "plt.legend()\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958ea93",
   "metadata": {},
   "source": [
    "- 위의 그래프는 시그모이드 함수의 그래프를 보여줍니다. 위 그래프를 시그모이드 함수의 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 완만해지는 모습을 볼 수 있습니다. 기울기가 완만해지는 구간을 주황색, 그렇지 않은 구간을 초록색으로 칠해보겠습니다.\n",
    "- 주황색 부분은 기울기를 계산하면 0에 가까운 아주 작은 값이 나오게 됩니다. 그런데 역전파 과정에서 0에 가까운 아주 작은 기울기가 곱해지게 되면, 앞단에는 기울기가 잘 전달되지 않게 됩니다. 이러한 현상을 기울기 소실(Vanishing Gradient) 문제라고 합니다.\n",
    "- 시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 됩니다. 다시 말해 매개변수 $W$ 가 업데이트 되지 않아 학습이 되지를 않습니다.\n",
    "- <u>결론적으로 시그모이드 함수를 은닉층에서 사용하는 것은 지양됩니다.</u>\n",
    "\n",
    "- 하이퍼볼릭탄젠트 함수도 -1과 1에 가까운 출력값을 출력할 때, 시그모이드 함수와 같은 문제가 발생합니다. 그러나 하이퍼볼릭탄젠트 함수의 경우에는 시그모이드 함수와는 달리 0을 중심으로 하고 있는데, 이때문에 시그모이드 함수와 비교하면 반환값의 변화폭이 더 큽니다. 그래서 시그모이드 함수보다는 기울기 소실 증상이 적은 편입니다. 그래서 은닉층에서 시그모이드 함수보다는 많이 사용됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71308e00",
   "metadata": {},
   "source": [
    "#### 렐루 함수(ReLU)\n",
    "- 인공 신경망에서 가장 최고의 인기를 얻고 있는 함수입니다. 수식은 $f(x)=\\max(0,x)$ 로 아주 간단합니다.\n",
    "- 렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환합니다. 렐루 함수는 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동합니다. 뿐만 아니라, 렐루 함수는 시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠릅니다.\n",
    "- 하지만 여전히 문제점이 존재하는데, 입력값이 음수면 기울기도 0이 됩니다. 그리고 이 뉴런은 다시 회생하는 것이 매우 어렵습니다. 이 문제를 <u>죽은 렐루(dying ReLU)</u> 라고 합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75afd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35251b6",
   "metadata": {},
   "source": [
    "#### 리키 렐루(Leaky ReLU)\n",
    "- 죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장하기 시작했습니다. 변형 함수는 여러 개가 있지만 여기서는 Leaky ReLU에 대해서만 소개합니다. Leaky ReLU는 입력값이 음수일 경우에 0이 아니라 0.001과 같은 매우 작은 수를 반환하도록 되어있습니다.\n",
    "\n",
    "- 수식은 $f(x)=\\max(ax,x)$ 로 아주 간단합니다. $a$ 는 하이퍼파라미터로 Leaky('새는') 정도를 결정하며 일반적으로는 0.01의 값을 가집니다. 여기서 말하는 '새는 정도'라는 것은 입력값의 음수일 때의 기울기를 비유하고 있습니다.\n",
    "\n",
    "#### 소프트맥스 함수(Softamx function)\n",
    "- 은닉층에서 ReLU(또는 ReLU 변형) 함수들을 사용하는 것이 일반적이지만 그렇다고 해서 앞서 배운 시그모이드 함수나 소프트맥스 함수가 사용되지 않는다는 의미는 아닙니다. 분류 문제를 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용하여 사용합니다.\n",
    "- 소프트맥스 함수는 시그모이드 함수처럼 출력층의 뉴런에서 주로 사용되는데, 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류 (Binary Classification) 문제에 사용된다면 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용됩니다.\n",
    "\n",
    "\n",
    "#### 참고사항\n",
    "\n",
    "- 시그모이드 함수의 또 다른 문제점은 원점 중심이 아니라는 점입니다(Not zero-centered). 따라서, 평균이 0이 아니라 0.5이며, 시그모이드 함수는 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높습니다. 이것을 <u>편향 이동(bias shift)</u>이라 하며, 이러한 이유로 각 레이어를 지날 때마다 분산이 계속 커져 가장 높은 레이어에서는 활성화 함수의 출력이 0이나 1로 수렴하게 되어 기울기 소실 문제가 일어날 수 있습니다.\n",
    "\n",
    "- 하이퍼볼릭탄젠트 함수는 원점 중심(zero-centered)이기 때문에, 시그모이드와 달리 편향 이동은 일어나지 않습니다. 하지만, 하이퍼볼릭탄젠트 함수 또한 입력의 절대값이 클 경우 -1이나 1로 수렴하게 되는데 시그모이드 함수와 마찬가지로 이때 기울기가 완만해지므로 역시나 기울기 소실 문제가 일어날 수 있습니다.\n",
    "\n",
    "- 스탠포드 대학교의 딥 러닝 강의 cs231n에서는 ReLU를 먼저 시도해보고, 그다음으로 LeakyReLU나 ELU 같은 ReLU의 변형들을 시도해보며, sigmoid는 사용하지 말라고 권장합니다.\n",
    "\n",
    "$$\n",
    "ELU(x) = \\left\\{\\begin{array}{ll} \\alpha (\\exp (-x) -1) \\qquad &\\text{if } x>0 \\\\ x & \\text{if } x\\le 0 \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f78f91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efd7cfc9430>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFpCAYAAACmt+D8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAn40lEQVR4nO3deXRc533e8efFSgBcBhs3cAUpiaJEiaKGWmI7sS3QSWyldmxSdpzEcRJnmNPGberGVJ2TpMnJaRwyjdumaWOO0xM50YnNpVGd1I5sMpYXSZZFEJRFkyJFAtwXYLAM9nXm7R+4Aw4Gg4WYwdw7M9/POTzEXcb3x+vBPHq3O8ZaKwAA4B0FbhcAAAAmI5wBAPAYwhkAAI8hnAEA8BjCGQAAjylyu4CYmpoau2HDBrfLyKhQKKTa2lq3y8hq3MP04D6mjnuYuny8hydPnmy31k75R3smnDds2KDGxka3y8gov9+fd//mdOMepgf3MXXcw9Tl4z00xlxJtp9ubQAAPIZwBgDAYwhnFwUCAbdLyHrcw/TgPqaOe5g67uEdxiuP7/T7/TbfxhoAAPnNGHPSWutP3E/LGQAAjyGcAQDwGMIZAACPIZwBAPAYwhkAAI8hnAEA8BjCGQAAjyGcAQDwGMIZAIBZ/LClQ8+9ckmjkWhGrkc4AwAwi3/+8W392TfPq6jAZOR6hDMAALO41N6vDTUVMoZwBgDAEy53jIdzphDOAADMYDQS1fWuQW2sJpwBAPCEa50DikQtLWcAALziSseAJGljTXnGrkk4AwAwg0vt/ZKk9Rns1i6a7QRjjE9SQFK1tfbZWc49aK3dm7Bvn6QWSVWSZK0NzrtaAAAy7HJHv5aUFqm6oiRj15yx5WyMaZDUIGmTJN8s5+6XVJ9kX4u19qgTypuMMbtTqhgAgAzK9DIqaZZwttYet9YelRSe6TxjzI5pDgWc18cckrR3mnMBAPCcTC+jktI35uyXdCx+xzSBHdZ4SxwAAM8bGYvqRtegNlZnbjKYlIZwdrqpDyc5VCWpM2Ff4jYAAJ51tXNAUavsajk7k8XC1tpwksO+WV43SSgUkt/vn/gTDDJvDADgrsvOTO10hnMwGJzIOkk1yc6Zdbb2LJ6ZYfZ1WM4M7TiJ2xNqa2vV2NiYYjkAAKTP5Y7xcE7n08ECgYACgYAkyRjTnuycebecnTHl4zOc0qmprWefJE3T0gYAwFMutfdrWVmxKjO4jEpKreVcJakhbmr5Tkn1zrrmo9baJmNMOMlrZgp0AAA8oyXUr/razI43SymEs7X2uOKC1hgTkFRvrT0Qd9phY8zuuOVUuyQdnO81AQDIpOZQn951T23GrzvbQ0h2OC3h3RpvJe9LtkTKCeY9clrOsQlfztPC6o0xDc45zQnrngEA8KTeoVG19Q5r03KPtZyttU2SmiQdmOW8oKSkE8MSWtIAAGSF2DO162sWZ/zafPEFAABJtITGw3mTC2POhDMAAEm0hPpUYKR1GX46mEQ4AwCQVHOoX+uqylVaVJjxaxPOAAAk0RzqU31t5sebJcIZAIApolGryx39qs/wM7VjCGcAABLc7B7U0GiUljMAAF7R7OJMbYlwBgBgipZQnyTRcgYAwCtaQv1asqhINYsz+4UXMYQzAAAJWtrHZ2rHfblTRhHOAAAkaG7rd228WSKcAQCYpH94TLd7hrTJpfFmiXAGAGCSO194QcsZAABPaHZ5prZEOAMAMElz2/gXXmyoyfwXXsQQzgAAxLnQ1qcN1RWufOFFDOEMAECcC2192rzcvS5tiXAGAGDCyFhUl9v7dc8KwhkAAE+43NGvsajVPcuXuFoH4QwAgONC6/hMbbq1AQDwiAttvTJGrj6ARCKcAQCYcKGtT2sry1VW4t5MbYlwBgBgwsXWPt3r8mQwiXAGAECSNBaJqqW9T5tdngwmEc4AAEiSrnQOaDRidY/Lk8EkwhkAAEl3Zmq7vcZZIpwBAJAkXWzrleT+TG2JcAYAQNL4TO06X5kqSovcLkWzVmCM8UkKSKq21j47zTFJ2inpmLU2mHDOPkktkqokKfE4AABecKG1zxNd2tIs4WyMaZDkk7RpmlM+Fx/YxphmY8xEABtj9ks6Ya09Gts2xuyObQMA4AWRqFVzqE/v2FztdimSZunWttYed4I0nHjMaTXXJ+w+KCm+dR1ICOJDkvbOq1IAABbItc4BDY9FXX+mdkyqY84Nxpj4gA7LCWxjzI4k54clNaR4TQAA0urt1vHJYJuzoVt7JtbasKTKhN27JB13fq6S1JlwPHF7QigUkt/vn9gOBAIKBALTnQ4AQNqcvz0ezveuWPiWczAYVDA4Mf2qJtk5aZuS5nRzN0h6ytnlm+lcJ9wn1NbWqrGxMV3lAAAwZ+dae7W2qkyLMzBTO77xaYxpT3ZOOpdSfUnSHmttk7MdljNDO07iNgAArjt/u1f3rVjqdhkT0hLOznKpg9ba43G7OzW19eyTJrrEAQBw3dBoRJfa+7VlpTcmg0lpCGdjzG5JTbFgdpZfyWlBhxNOr9KdMWkAAFx3sa1PkajVfbkSzk4QV0lqNMb4nJnb8bO0DzvhHbNL48utAADwhNhkMC+1nGd7CMkOjU/y2u1s75N03Frb5EwAO+acGh+4E+uarbV7jTH7nBCvl9TMA0gAAF5yvrVXJYUF2lhT4XYpE2YMZ6druknSgSTHwpLMbBew1k55LQAAXnHudq82L1+sokLvfN2EdyoBAMAF52/3eKpLWyKcAQB5LDwwotaeYU9NBpMIZwBAHjvnTAYjnAEA8Ig7M7W98wASiXAGAOSxc7d7taysWCuWlrpdyiSEMwAgb52/3aP7Vi6RMbMuPsoowhkAkJestXq7tc9zM7UlwhkAkKeudw2qb3jMc5PBJMIZAJCnztzskSQ9sHqZy5VMRTgDAPLS2ZvdKjDSfStoOQMA4Alnb/VoU+1ilZUUul3KFIQzACAvnbnZo62rvbW+OYZwBgDknc7+Ed3qHtIDhDMAAN7w1q3xyWBbV3lvMphEOAMA8tCZm92SRLc2AABecfZmj1YtW6SqihK3S0mKcAYA5J0zN3u0dZU3W80S4QwAyDNDoxE1h/o8OxlMIpwBAHnm/O1eRa13x5slwhkAkGdij+306kxtiXAGAOSZs7e6taS0SGurytwuZVqEMwAgr5y52aP7Vy/13Hc4xyOcAQB5YywS1blbvZ6eDCYRzgCAPNIc6tfgaETb6rw73iwRzgCAPHL6xviTwR5aQzgDAOAJp6+HVVFSqI01i90uZUaEMwAgb7x5o1sP1C1TYYF3J4NJUtFsJxhjfJICkqqttc8mOb5PUoukKkmy1gbv5jgAAJkwGonq7M0e/fIT690uZVYztpyNMQ2SGiRtkuRLcny/pBZr7VEndDcZY3bP9TgAAJlyobVPw2NRbfP4eLM0Szhba49ba49KCk9zSsA5HnNI0t67OA4AQEacvhGWJD20xudqHXMx7zFnY8yOJLvDGm9pz3ocAIBMevP6+JPB1leVu13KrFKZEFYlqTNhX+ddHAcAIGN+fKNbD9YtU4HHJ4NJqYWzb7oDziSy2Y5PEgqF5Pf7J/4Eg8wbAwCkx8hYVG/d6vXE+uZgMDiRdZJqkp0z62ztGYTlzMCOU3UXxyepra1VY2NjCuUAAJDc2629Gol4YzJYIBBQIBCQJBlj2pOdk0rLuVNTW8c+SbLWhudwHACAjHjzuvNksDqfu4XM0bzD2VrbpKmzuKskHZ/LcQAAMuX0jbCWlRV7+msi46X6hLDDCeuWd0k6eBfHAQBYcD+61q1tdcs8/TWR8WZ7CMkO5wlfuyU1GGP2xS+RstbulVRvjGkwxgQkNceva57tOAAAC21gZEznW3v1yDqf26XM2YwTwpyu6SZJB2Y4Z9pjczkOAMBCOn29W5Go1fa1PrdLmTO++AIAkNPeuBaWJMIZAACvOHU1rPXV5apeXOp2KXNGOAMActqpa11Z1WqWCGcAQA671T2o1p5hPUI4AwDgDaeuhiVJj6yrdLeQu0Q4AwBy1qmrXSopKtD9q5a6XcpdIZwBADnrjWthPbh6qUqKsivusqtaAADmaDQS1ZvXu7OuS1sinAEAOer87V4Nj0Wz6slgMYQzACAnnbraJSm7Hj4SQzgDAHJS09WwapeUqs6XHd9EFY9wBgDkpMYrnfKvr8yab6KKRzgDAHJOa8+QrnUO6tH12TcZTCKcAQA5qPHy+Hjzzg1VLlcyP4QzACDnnLjcqbLiQm1dnV0PH4khnAEAOafxSqe2r/WpuDA7Yy47qwYAYBp9w2M6e7NHOzdk53izRDgDAHLMG1fDilrJn6XjzRLhDADIMScud6rAKCufDBZDOAMAckrjlU5tWblUSxYVu13KvBHOAICcMRaJ6tTVsPxZPN4sEc4AgBzy1q1eDYxEsnq8WSKcAQA55PXLnZIkf5Y+GSyGcAYA5IwftnRobVWZVmfhl13EI5wBADkhGrX64aVOPbGx2u1SUkY4AwBywrnbveoeHNUT9YQzAACe8FpLhyTp8frsngwmSUXp+B8xxuyTFHY2fdbaA0mOt0iqkiRrbTAd1wUAIOY1Z7x5TWW526WkLOWWszFmn7X2gLU26ITucSeMY8f3S2qx1h51jm8yxuxO9boAAMTk0nizlJ5u7Y/Gb1hrmyTtjNsVsNYejds+JGlvGq4LAICk3BpvltITzp3GmCOxDWNMQOMBLGPMjiTnhyU1pOG6AABIyq3xZik9Y857JZ00xnRJ+rycLmznWJWkzoTzE7cBAEhJLo03S2loOVtrWzQeyo2S9mtyl7ZvutcZYyYdC4VC8vv9E3+CQeaMAQBml23jzcFgcCLrJNUkOyfllrMx5qCkg9baA8aYBklHjDH11to9Gu/CTuxjSNrnUFtbq8bGxlTLAQDkmWwbbw4EAgoEApIkY0x7snNSCmdnTDnsTAKTtfa4MWajpEvOKZ2a2nr2OeeGU7k2AACS9GrzeL49uSk7wnkuUu3WrpLUEb/DCd3jzs9NurP+Of41x1O8LgAAkqSXL7arvrYi65+nHS+lcLbWHpe0K36fM5bcErfrcMK65l2SDqZyXQAAJGl4LKIftnTqnZuTDt1mrbTM1nYeNNIc22GtfTbu573GmH3OeHS9pOaEdc8AAMzLqathDY5GCOdEzmztZ2c558BMxwEAmI+XL7SrsMDoiRwab5b44gsAQBb7/sV2PbxmmZYuKna7lLQinAEAWal7YFSnr4f1zntq3S4l7QhnAEBW+kFLu6JWetc9uTXeLBHOAIAs9f0L7aooKdT2tT63S0k7whkAkJVeudiuJ+qrVVyYe1GWe/8iAEDOu9Y5oMsdA3pHji2hiiGcAQBZ5ztvhyRJP3Vf7k0GkwhnAEAW+s65Nq2rKld9TYXbpSwIwhkAkFWGRiN6pbld77mvVsYYt8tZEIQzACCrvH6pU0OjUb37vuVul7JgCGcAQFZ56XybSosKsub7m+eDcAYAZJXvnA/pyU3VKispdLuUBUM4AwCyxuX2fl1q79e7783NWdoxhDMAIGt853ybJOX0eLNEOAMAsshL50Oqr6nQhhxdQhVDOAMAssLAyJhea+nI2QePxCOcAQBZ4fsX2jU8FtWu+1e4XcqCI5wBAFnhW2datXRRkXZurHK7lAVHOAMAPG8sEtW3z7XqvVuW5+S3UCXK/X8hACDrNV7pUtfAqN73wEq3S8kIwhkA4HnHzraqpLBAP5nj65tjCGcAgKdZa3XsbKt+YnO1FpcWuV1ORhDOAABPO9/aq6udA3rf1vzo0pYIZwCAxx070ypJarg/t58KFo9wBgB42rfOtmr7Wp+WL13kdikZQzgDADzrVvegTt/o1vseyP0Hj8QjnAEAnvXij29LUl6NN0tSWqa9GWN8kj4nqdnZ1WitbYo7vk9Si6QqSbLWBtNxXQBAbvv6m7e0ZeUSbV6+2O1SMirllrMTzEestc/Ghe7n4o7vl9RirT3qHN9kjNmd6nUBALntVvegGq906emHVrldSsalo1v7S5IOxm0flvRs3HbAWns0bvuQpL1puC4AIId94/R4l/b7txHO87Fb0nFjTL0xZoe1NmytbZEkY8yOJOeHJTWk4boAgBz29Tdvauuqpaqvza8ubSnFcI4LX3/cviNOV7c0PsbcmfCyxG0AACa5ER5U09WwPpCHXdpS6i3n+tgP1toWZxLYIY13dUuSb7oXxgW4JCkUCsnv90/8CQaZMwYA+eqfT9+SJH0gB7u0g8HgRNZJqkl2TqqztcPO341x+1o03tUdO574xZtJv4iztrZWjY2NyQ4BAPLM/3vzlh6sW6oNNRVul5J2gUBAgUBAkmSMaU92Tqot5xZJstaG4/aFnQv6NN6F7Ut4jS/JawAAkCRd6xzQG9fC+sC21W6X4pqUwtmZ+BVO6KL2SQo7E8OadKd1HVMl6Xgq1wUA5K5v5HCX9lylY7b25yU9E7f9UWdfzOGEdc27NHnpFQAAE144dUPb1/q0rrrc7VJck3I4W2sPSPIZY/Y5TwLrcPbFju+VVG+MaTDGBCQ1J6x7BgBAkvTWrR6du92rD++oc7sUV6Xl8Z3xYTyf4wAASOOt5qICo6cfyt/xZokvvgAAeEQkavW1N27o3fctV1VFidvluIpwBgB4wqvN7WrtGc77Lm2JcAYAeMQ/NN3QkkVFeu+W5W6X4jrCGQDguv7hMb3449t6+qFVWlRc6HY5riOcAQCu++aZ2xocjejnH1njdimeQDgDAFz3D003tKayTP71lW6X4gmEMwDAVdc6B/TyxXbteXStCgqM2+V4AuEMAHDVoRPXVGCkZ3bSpR1DOAMAXDMWierIyWt6933LtWpZmdvleAbhDABwzUvnQ2rtGdbHdq51uxRPIZwBAK756utXVbukVO9hbfMkhDMAwBW3ugf10vk27Xl0jYoLiaN43A0AgCuONF5X1EofpUt7CsIZAJBxkajVoRPX9I7N1VpfXeF2OZ5DOAMAMu6lc226ER7Uxx9b73YpnkQ4AwAy7rlXL2vVskV63wMr3C7FkwhnAEBGXWzr1csX2/VLT6xnItg0uCsAgIz68qtXVFJUwNrmGRDOAICM6Rka1f9puq6fe2i1qheXul2OZxHOAICMOdJ4XQMjEX3yJza4XYqnEc4AgIyIRq3+9geX9ej6Sm1bs8ztcjyNcAYAZMS3z7XpSseAfoVW86wIZwBARhz8XrPqfGX62QdXul2K5xHOAIAFd/JKp05c7tKn3rWR5VNzwB0CACy4L363Rb7yYp6jPUeEMwBgQV1s69Wxs636xJMbVF5S5HY5WYFwBgAsqOD3WrSouEC/8iTP0Z4rwhkAsGBudw/phVM39Ix/LQ8duQtp718wxhy01u5N2LdPUoukKkmy1gbTfV0AgPf89fdbFIla/ca76t0uJaukteVsjNkvqT7JvhZr7VEnlDcZY3an87oAAO9p6x3S8z+8og89Uqe1VeVul5NV0hbOxpgd0xwKWGuPxm0fkrR3mnMBADki+N0WjYxF9en33uN2KVknnS1nv6Rj8TumCeywpIY0XhcA4DHxreaNNRVul5N10hLOTjf14SSHqiR1JuxL3JYkhUIh+f3+iT/BIMPSAJCtDn63RaMRS6s5iWAwOJF1kmqSnZPyhDBjjE9S2FobNsYkHvbN9DprbTi2XVtbq8bGxlTLAQC4rK13SM+/dkUf2k6rOZlAIKBAICBJMsa0JzsnHS3nZ6y1x6c5FpYzQztO4jYAIId88TstGotaffq9m90uJWulFM7OmPJ0wSyNd2H7Evb5JCm+1QwAyA3Xuwb0/GtX9POP1GkDreZ5S7Vbu0pSQ1x39k5J9c665qPW2iZjTDjJa2YKdABAlvrCt96WjPTvd93rdilZLaVwdrqzJ4LWGBOQVG+tPRB32mFjzO645VS7JB1M5boAAO85e7NHL7xxQ4F31avOV+Z2OVktneucA5L2yGk5OxPF5DwtrN4Y0+Cc05yw7hkAkAP+9MVzWrqoWP/63Yw1pyptj+90nv6VdP1TQksaAJBjXrnYru+9HdLvvn+LlpUXu11O1uOLLwAAKYlGrf70n8+pzlemTzy5we1ycgLhDABIydGT13X6Rrd+56fv1aLiQrfLyQmEMwBg3roHR7X/xXPasc6nD22vc7ucnJH2r4wEAOSPv/iXC+ocGNGXf+0xJXlKJOaJljMAYF4utPbqy69e1sd2rtODdcvcLienEM4AgLtmrdUf/tMZlZcU6nfexwNH0o1wBgDcta+fvqVXLnboP7zvPlUvLnW7nJxDOAMA7kr3wKj+8B/PalvdMv3i4+vcLicnMSEMAHBX/uQbb6lrYERf/rWdKiqkjbcQuKsAgDn7QXOHDjVe06fetVEPrGYS2EIhnAEAczI0GtHvvnBa66rK9dtPMQlsIdGtDQCYk/96/G1dau/X87/+uMpKeBLYQqLlDACY1YnLnQp+r0Uf27lW77ynxu1ych7hDACYUd/wmD5z+A2tqSzT7z291e1y8gLd2gCAGf3nr5/V9a5BHd77pBaXEhuZQMsZADCtb59r1Vdev6bAT9Zr54Yqt8vJG4QzACCp1p4hffbIm9qycok+s4vZ2ZlEOAMAphiLRPVvv3JKAyMR/eXHH1FpEbOzM4nBAwDAFH/xLxf0w0ud+vM9D2vz8iVul5N3aDkDACZ5+UK7/sdLF7Xn0TX6yKNr3C4nLxHOAIAJN8OD+u1Dp7S5drH+6IMPuF1O3iKcAQCSxh/PuffvTmpoNKq/+qUdKi9h5NMt3HkAgKy1+tw/nNbpG9360if8jDO7jJYzAED/++VLeuHUDX1m173atXWF2+XkPcIZAPLcS+fa9CffeEs/88BK/dZ7NrtdDkQ4A0BeO329W//m75t0/6ql+vNnHlZBgXG7JIhwBoC8da1zQL/63AlVlpfobz65UxU8N9szUv5/whjjkxRwNndKOmatDSacs09Si6QqSUo8DgDIrO6BUX3yb17XyFhEXw08ruVLF7ldEuKk4z+TPmetfTa2YYxpNsZMBLAxZr+kE9bao7FtY8zu2DYAILP6h8f0yede17XOQf3drz/GzGwPSqlb22k11yfsPijp2bjtQEIQH5K0N5XrAgDmZ2g0ok99uVFvXu/WX/zCI3q8vtrtkpBEOsacG4wx8QEdlhPYxpgdSc4PS2pIw3UBAHdhZCyq33z+pF671KE/3/OwfubBlW6XhGmk1K1trQ1LqkzYvUvScefnKkmdCccTtwEAC2w0EtW/++opfed8SJ//8DZ96JE6t0vCDNI6Nc/p5m6Q9JSzyzfTuU64S5JCoZD8fv/E8UAgoEAgkOylAIC7MDwW0af//pS+dbZVv//0Vv3CY+vcLimvBYNBBYMT86Jrkp1jrLVpu6Ax5oikg9ba4852g6Qj1trKuHPqJTVLqowPZ7/fbxsbG9NWCwBAGhyJaO/zJ/W9t0P6w5/bqk++Y6PbJSGOMeaktdafuD9tLWdnudREMDs6NbX17JMmusQBAAukb3hMv/7cCb1+uVP7P7JNH91JizlbpOUhJMaY3ZKaElrMstY2aXwCWLwq3RmTBgAsgPa+Yf3il15T45Uu/bePbieYs0zK4ewEcZWkRmOMz+m2jp+lfdgJ75hdGl9uBQBYAM2hPn34f72q8629+uIvPaoPbmfyV7ZJqVvbmQB2zNmMD9yJdc3W2r3GmH1OiNdLauYBJACwME5c7tRv/G2jCo3RV37jCT2yLnFBDbJBOpZSzfqUdGvtgVSuAwCY3dfeuKHPHn1Tdb4yPferO7W+usLtkjBPPOUcALLcWCSq/S+e05e+f0mPbajSwV9+VJUVJW6XhRQQzgCQxbr6R/RbX2nSKxc79Ikn1+v3PrBVJUV84WC2I5wBIEs1Xe3Sp//+lEK9wzqw+yE941/rdklIE8IZALJMNGr1V99t1heOva2VSxfp8G8+qe1rfW6XhTQinAEgi7T2DOkzh9/QKxc79IGHVulPfn6blpUVu10W0oxwBoAsYK3VN07f1u9/7ccaHIlo/0e26Rn/Whkz64IZZCHCGQA8rq13SH/wf8/oxTO39dCaZfrCMw9r8/IlbpeFBUQ4A4BHWWv1wqkb+qN/OqvB0Yj+489u0afeuVFFhczGznWEMwB40NutvfpPXzujH7R0aMc6nw7sflibly92uyxkCOEMAB7SOzSq/378gp579bIqSov0xx98QB9/fL0KCxhbzieEMwB4wFgkqiMnr+sLx95We9+wPrZzrT7701tUxZO+8hLhDAAustbqxR/f1p9967xaQv3asc6nv/6EXw+zbjmvEc4A4AJrrV5t7tCBb57Xj66FtXn5YgV/+VHt2rqC5VEgnAEgk6y1+va5Nv3lSxd16mpYq5Yt0oGPPKQP76hjFjYmEM4AkAFjkahePHNb//OlZr11q0d1vjL98Yce1J5H12hRcaHb5cFjCGcAWEBd/SP66olrev61K7oRHlR9bYX+y56H9cHtq1VMSxnTIJwBYAG8datHX371sl44dUPDY1E9WV+t3396q3ZtXcGyKMyKcAaANAkPjOgff3RTR09e15vXu7WouEAf3rFGv/IT67Vl5VK3y0MWIZwBIAVjkahevtiuIyev69iZVo1Eorp/1VL9wdNb9eEddfKVs04Zd49wBoC7NBaJ6gctHfrG6Vv65plWdfaPqLK8WB9/fJ32+NfogdXL3C4RWY5wBoA5GBqN6AfNHfrmmdv65pnb6hoYVUVJod57/wp9YNsqvWdLrUqLmHWN9CCcAWAa1zoH9NL5Nr10rk2vNndoeCyqxaVFeur+5Xr/tlX6qXtrWQaFBUE4A4Cjo29Yr1/q1GstHXr5YruaQ/2SpA3V5fr44+v0nvuW67GNVQQyFhzhDCBvtfUM6eSVLr3W0qHXWjp1vrVXklRWXCj/hkr94uPr9Z4ty7WxpsLlSpFvCGcAeWFwJKLTN7r1xrUuvXEtrDeuhnWze0jSnTD+V9tX64n6aj20ZhkPCIGrCGcAOae9b1jnbvXqrVs9eut2j9661au3W3sViVpJ0prKMu1YX6lfW+vTI+sqCWN4DuEMICtZa9XRP6JL7f1qCfWpOdSvt2716NztXoV6hyfOW7G0VPetXKqntizX9rU+PbzWp9olpS5WDswuI+FsjNknqUVSlSRZa4OZuC6A7GatVXvfiG6EB3W9a0CX2/vVEupXixPIPUNjE+eWFBXo3hWL9VP31ur+VUt1/8ol2rJqqaoqeAgIss+Ch7MxZr+kE9bao7FtY8zu2DaA/DUwMqa2nmG19Q7rZnjQCeHxIL4RHtSNrkENj0UnvWbVskWqr63QB7fXaWNNheprK7SpdrFW+8p4ZjVyRiZazgFr7bNx24ck7ZdEOAM5xlqrgZGIugZG1NU/qq6BEXX0D08EcFvvsNp6hhRyfu4bHpvyv1FdUaK6yjJtWblET21ZrjWV5arzlamuskzrq8tVXsJoHHLfgr7LjTE7kuwOS2pYyOsCmJ+xSFT9wxH1j4ypf3hM/SOR8b+Hx5x949u9Q2PqHBhROC6Ex/+MaiShpRtTVlyo5UtLtXxJqe5ftVQ/eW+ps71Iy5eUarVvkVb7yghfQAvfcq6S1JmwL3EbwDxYazU0GlXf8JgGRsacvyPjfzshOjVkJ++fON/5e7pgTVRgJF95iSrLi1VZXqI1leV6aM0yVZaXqLJifL+vvERVFeN/li8p1eLSIhlDtzMwFwsdzr7pDhhjfNbacGw7FArJ7/dPHA8EAgoEAgtaHJBJY5Go+kciGogF5kRQJgRpwv5Y8Ma3aAecn52VQbMqKjCqKC3S4tIilZcUqqK0SBWlhapZXDrxc0VJkSqc44tLi1ReWqTFpYUqL7nzutj+8uJCFTC+C8xLMBhUMDgxL7om2TnG2jn+ds+DMaZB0hFrbWXcvnpJzZIq48PZ7/fbxsbGBasFuBvWWg2PRSdaoclbp4ndvpGkXcGx1ydObJpJeUksFOPCsdQJ1Vi4OmF6J1gLnXBNDNNCvpAB8ChjzElrrT9x/0K3nDs1tfXsk6T4YAZSFYnaJC3P8Z8nQnVSyCa0YBN+HhiJTDywYjaxVmksNGMtzqqK8skhGRegU0P2zv6y4kJmHQN5bkHD2VrbZIwJJ+yuknR8Ia8Lb4u1SqcEY3yLc1K3bkKoDt9pwcZCeGh07q3SsuLChFZmoaoqSrS2slwVCS3VxXHnlE9pqY7/XFJYwFgqgLTKxLTIwwnrmndJOpiB6yJNIlHrtDIjs7dO47p8J7VOnSCNnTPXVmlhgbnTwoxrba4pLx8PzNKi5K3T+CCNa52WlxTRKgXgeQseztbavcaYfc74c72kZh5AsnBirdKBWSYZJe6f0jqNOz44Gpnz9RcVF8SF5HiY+spLVFc5OTCTTTJKnHxUUVqk0iJapQDyT0YWFFprD2TiOtko6oyVxi+BSZx8NHVcdOrko/iu3rE5tkoLjCbN4I2Fap2vJK7FmXzy0eQuX2eyEq1SAEgLVvvfpeGxSFyA3gnS/rhu3MSQTTbhKNZivdtWaeJyl2VlxarzLZrS4pzUFTzN5CNapQDgTTkdztGo1cDo5AlEs00sShxLTezyHY3cXat00kzckiKt9hVPmoiUbPJRsjHT8uJCFfGVdgCQF3IynL/+5i199uiPNDAy91ZpaVHBpO7ZitIiLS0r1qpli6a2REuST0SKD1lapQCA+crJcF5fXa6PP7YuyWzd+JC9s5SmooRWKQDAO3IynB+sW6YH65a5XQYAAPNCcxEAAI8hnAEA8BjCGQAAjyGcAQDwGMIZAACPIZwBAPAYwhkAAI8hnAEA8BjCGQAAjyGcAQDwGMLZRcFg0O0Ssh73MD24j6njHqaOe3gH4ewi3oip4x6mB/cxddzD1HEP7yCcAQDwGGOtdbsGSZIxJiTpitt1ZFiNpHa3i8hy3MP04D6mjnuYuny8h+uttbWJOz0TzgAAYBzd2gAAeAzhDACAxxDOAAB4DOEMAIDHEM4AAHhMkdsFYCpjzEFr7V6368gmxhifpICzuVPSMWstTzSYhTFmn6QWSVWSxD27O7zv0o/Pv3GEs8cYY/ZLqne7jiz0OWvts7ENY0yzMYawmYHzXjthrT0a2zbG7I5tY05436URn3930K3tIcaYHW7XkI2c1kviL/RBSc9OPRtxAglBfEhS3rdY5or3XXrx+TcZ4ewtfknH3C4iSzUYY+I/KMPiv8CnNc0HYVhSQ4ZLyXa879KHz784hLNHGGN2Szrsdh3ZyFobttZWWmtb4nbvknTcrZqyQJWkzoR9iduYAe+79OHzbyrGnD3A6R4LW2vDxhi3y8l6zv1skPSUy6V4mW+6A8YYn7U2nLlScgPvu/nh8y85Ws7e8Iy1lv/aTp8vSdpjrW1yuxAPC8uZoR0ncRt3h/fd/PD5lwQt5wXgdNF8dJbTOq21e52xP96YCe7mHia8bp+kg/yyz6pTU1vPPmm8uzbDtWQ93nfzw+ff9PhWKpcZYxokxU/O2elsH5R0NGE8CzNwAj0c+4A0xjTwYTk9Y0yXtbYybrtB0rPW2l0ulpV1eN/NH59/06Pl7DLnl3jiF9kYE5BUb6094F5V2cf5Ja+SdNwZw6rS+C85H5LTO5ywrnmXxj8UMUe871LD59/0aDl7iPPG3KPxJQWflxSki3F2zodiV5JDR621ezJcTlZxumOb5Cz/4eEZc8f7Lr34/JuMcAYAwGOYrQ0AgMcQzgAAeAzhDACAxxDOAAB4DOEMAIDHEM4AAHgM4QwAgMcQzgAAeAzhDACAx/x/1251mNS6v5sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu0(x, alpha):\n",
    "    if x> 0 :\n",
    "        return alpha*(np.exp(x)-1.)\n",
    "    else :\n",
    "        return x\n",
    "elu = np.vectorize(elu0)\n",
    "x2=np.arange(-5.0, 5.0, 0.01)\n",
    "y2 = elu(x2, 1.0)\n",
    "fig=plt.figure(figsize=(8, 6))\n",
    "plt.plot(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63049f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95004e1a",
   "metadata": {},
   "source": [
    "## 다중 퍼셉트론으로 손글씨 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b284d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00107682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total images are 1797.\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits() \n",
    "print(\"The number of total images are {}.\".format(len(digits.images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cedc039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 상위 5개의 샘플 시각화.\n",
    "\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:5]): # 5개의 샘플만 출력\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('sample: %i' % label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb132f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 인덱스 샘플의 레이블 :  0\n",
      "1 번 인덱스 샘플의 레이블 :  1\n",
      "2 번 인덱스 샘플의 레이블 :  2\n",
      "3 번 인덱스 샘플의 레이블 :  3\n",
      "4 번 인덱스 샘플의 레이블 :  4\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개 샘플의 레이블을 확인.\n",
    "for i in range(5):\n",
    "    print(i,'번 인덱스 샘플의 레이블 : ',digits.target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1e19d",
   "metadata": {},
   "source": [
    "#### 다중 퍼셉트론을 이용한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ca1fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 2.334033\n",
      "Epoch   10/100 Cost: 2.076259\n",
      "Epoch   20/100 Cost: 1.809229\n",
      "Epoch   30/100 Cost: 1.510033\n",
      "Epoch   40/100 Cost: 1.199294\n",
      "Epoch   50/100 Cost: 0.905360\n",
      "Epoch   60/100 Cost: 0.659576\n",
      "Epoch   70/100 Cost: 0.478506\n",
      "Epoch   80/100 Cost: 0.357277\n",
      "Epoch   90/100 Cost: 0.277255\n"
     ]
    }
   ],
   "source": [
    "X = digits.data # 이미지. 즉, 특성 행렬\n",
    "Y = digits.target # 각 이미지에 대한 레이블\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(64, 32), # input_layer = 64, hidden_layer1 = 32\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16), # hidden_layer2 = 32, hidden_layer3 = 16\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10) # hidden_layer3 = 16, output_layer = 10\n",
    ")\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.int64)\n",
    "loss_fn = nn.CrossEntropyLoss() # 이 비용 함수는 소프트맥스 함수를 포함하고 있음.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X) # forwar 연산\n",
    "    loss = loss_fn(y_pred, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, 100, loss.item()))\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3424d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7efd7a1bd660>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e5072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b3eda6d",
   "metadata": {},
   "source": [
    "## 09. 과적합(Overfitting)을 막는 방법들\n",
    "\n",
    "학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터. 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다. 이는 모델이 학습 데이터를 불필요할 정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다. 이번 챕터에서는 모델의 과적합을 막을 수 있는 여러가지 방법에 대해서 논의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c196d",
   "metadata": {},
   "source": [
    "#### 1) 데이터의 양을 늘리기\n",
    "- 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어납니다. 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.\n",
    "\n",
    "- 만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식시킵니다.\n",
    "\n",
    "#### 2) 모델의 복잡도 줄이기\n",
    "- 인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정됩니다. 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것 입니다. \n",
    "- 인공 신경망에서는 모델에 있는 매개변수들의 수를 모델의 수용력(capacity)이라고 하기도 합니다.\n",
    "\n",
    "#### 3) 가중치 규제(Regularization) 적용하기\n",
    "\n",
    "- 잡한 모델이 간단한 모델보다 과적합될 가능성이 높습니다. 그리고 간단한 모델은 적은 수의 매개변수를 가진 모델을 말합니다. 복잡한 모델을 좀 더 간단하게 하는 방법으로 **가중치 규제(Regularizaiton)** 가 있습니다.\n",
    "\n",
    "  1. L1 규제 : 가중치 $w$들의 절대값 합계를 비용 함수에 추가합니다. L1 노름이라고도 합니다.\n",
    "  2. L2 규제 : 모든 가중치 $w$들의 제곱합을 비용 함수에 추가합니다. L2 노름이라고도 합니다.\n",
    "\n",
    "- L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda |w|$ 를 더 한 값을 비용 함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\dfrac{1}{2}\\lambda w^2$를 더 한 값을 비용 함수로 합니다. $\\lambda$ 는 규제의 강도를 정하는 하이퍼파라미터입니다. $\\lambda$ 가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 됩니다.\n",
    "\n",
    "- 이 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 $w$ 들의 값이 작아져야 한다는 특징이 있습니다. L1 규제로 예를 들어봅시다. L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 합니다. 이렇게 되면, 가중치 $w$의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 됩니다.\n",
    "\n",
    "- 예를 들어 $H(x)=w_1x_1+w_2x_2+w_3x_3+w_4x_4$ 라는 수식이 있다고 해봅시다. 여기에 L1 규제를 사용하였더니, $w_3$의 값이 0이 되었다고 해봅시다. 이는 $x_3$ 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미합니다.\n",
    "\n",
    "- L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화하므로 $w$ 의 값이 완전히 0이 되기보다는 0에 가까워지기는 경향을 띕니다. <u>L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용합니다. 만약, 이런 판단이 필요없다면 경험적으로는 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장합니다.</u> 인공 신경망에서 L2 규제는 가중치 감쇠(weight decay)라고도 부릅니다.\n",
    "\n",
    "- 파이토치에서는 옵티마이저의 weight_decay 매개변수를 설정하므로서 L2 규제를 적용합니다. weight_decay 매개변수의 기본값은 0입니다. weight_decay 매개변수에 다른 값을 설정할 수도 있습니다.\n",
    "\n",
    "~~~\n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "~~~\n",
    "\n",
    "- 책에 따라서는 Regularization를 정규화로 번역하기도 하지만, 이는 정규화(Normalization)와 혼동될 수 있으므로 규제 또는 정형화라는 번역이 바람직한 것 같습니다. 정규화에 대한 설명은 링크 : http://blog.naver.com/angryking/221330145300 를 참고.\n",
    "\n",
    "- 인공 신경망에서 정규화(Normalization)라는 용어가 쓰이는 기법으로는 또 배치 정규화, 층 정규화 등이 있습니다.\n",
    "\n",
    "#### 4) 드롭아웃(Dropout)\n",
    "- 드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법입니다. 예를 들어 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용합니다.\n",
    "\n",
    "- 드롭아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적입니다. 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e34508",
   "metadata": {},
   "source": [
    "## 10. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\n",
    "\n",
    "- 깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있습니다. 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 됩니다. 이를 기울기 소실(Gradient Vanishing)이라고 합니다.\n",
    "\n",
    "- 반대의 경우도 있습니다. 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 합니다. 이를 기울기 폭주(Gradient Exploding)이라고 하며, 뒤에서 배울 순환 신경망(Recurrent Neural Network, RNN)에서 발생할 수 있습니다.\n",
    "\n",
    "- 이번 챕터에서는 기울기 소실 또는 기울기 폭주를 막는 방법들에 대해서 다룹니다.\n",
    "\n",
    "#### 1) ReLU와 ReLU의 변형들\n",
    "- 앞에서 배운 내용을 간단히 복습해봅시다. 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워집니다. 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있습니다.\n",
    "\n",
    "- 기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것입니다.\n",
    "\n",
    "   - 은닉층에서는 시그모이드 함수를 사용하지 마세요.\n",
    "   - Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결합니다.\n",
    "   - 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용하세요.\n",
    "\n",
    "#### 2) 가중치 초기화(Weight initialization)\n",
    "\n",
    "- 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 합니다. 다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화시킬 수 있습니다.\n",
    "\n",
    "<b>1. 세이비어 초기화(Xavier Initialization)</b>\n",
    "\n",
    "논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "- 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했습니다. 이 초기화 방법은 제안한 사람의 이름을 따서 세이비어(Xavier Initialization) 초기화 또는 글로럿 초기화(Glorot Initialization)라고 합니다.\n",
    "\n",
    "- 이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세웁니다. 이전 층의 뉴런의 개수를 $n_{in}$, 다음 층의 뉴런의 개수를 $n_{out}$ 이라고 해봅시다.\n",
    "\n",
    "- 글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 합니다.\n",
    "\n",
    "$$\n",
    "W \\sim \\operatorname{Uniform} \\left(- \\sqrt{\\dfrac{6}{n_{in}+n_{out}}},\\, +\\sqrt{\\dfrac{6}{n_{in}+n_{out}}}\\right)\n",
    "$$\n",
    "\n",
    "- 다시 말해 $m=\\sqrt{\\dfrac{6}{n_{in}+n_{out}}}$ 일 때,  $-m$과 $+m$ 사이의 균등 분포를 의미합니다. 정규 분포로 초기화할 경우에는 평균이 $0$이고, 표준 편차 $\\sigma$ 가 다음을 만족하도록 합니다.\n",
    "$$\n",
    "\\sigma=\\sqrt{\\dfrac{2}{n_{in}+n_{out}}}\n",
    "$$\n",
    "\n",
    "- 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막습니다. 그런데 <u>세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않습니다.</u> ReLU 함수 또는 ReLU의 변형 함수들을 활성화 함수로 사용할 경우에는 다른 초기화 방법을 사용하는 것이 좋은데, 이를 He 초기화(He initialization)라고 합니다.\n",
    "\n",
    "<b>2. He 초기화(He initialization)</b>\n",
    "\n",
    "논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf\n",
    "\n",
    "- He 초기화(He initialization)는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉩니다. 다만, He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않습니다. 전과 같이 이전 층의 뉴런의 개수를 $n_{in}$ 이라고 해봅시다.\n",
    "\n",
    "- He 초기화는 균등 분포로 초기화 할 경우에는 다음과 같은 균등 분포 범위를 가지도록 합니다.\n",
    "\n",
    "$$\n",
    "W \\sim \\operatorname{Uniform}\\left( -\\sqrt{\\dfrac{6}{n_{in}}},\\,  +\\sqrt{\\dfrac{6}{n_{in}}} \\right)\n",
    "$$\n",
    "\n",
    "- 정규 분포로 초기화할 경우에는 표준 편차 σ가 다음을 만족하도록 합니다.\n",
    "\n",
    "$$\n",
    "\\sigma=\\sqrt{\\dfrac{2}{n_{in}}}\n",
    "$$\n",
    "\n",
    "   - 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적입니다.\n",
    "   - ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적입니다.\n",
    "   - ReLU + He 초기화 방법이 좀 더 보편적입니다.\n",
    "\n",
    "\n",
    "#### 3) 배치 정규화(Batch Normalization)\n",
    "\n",
    "- ReLU 계열의 함수와 He 초기화를 사용하는 것만으로도 어느 정도 기울기 소실과 폭주를 완화시킬 수 있지만, 이 두 방법을 사용하더라도 훈련 중에 언제든 다시 발생할 수 있습니다. 기울기 소실이나 폭주를 예방하는 또 다른 방법은 배치 정규화(Batch Normalization)입니다. 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만듭니다.\n",
    "\n",
    "<b>1. 내부 공변량 변화(Internal Covariate Shift) </b>\n",
    "\n",
    "- 배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있습니다. 내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말합니다. 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생합니다. 배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장합니다.\n",
    "\n",
    "   - 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.\n",
    "   - 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미합니다.\n",
    "\n",
    "<b>2. 배치 정규화(Batch Normalization)</b>\n",
    "\n",
    "- 배치 정규화(Batch Normalization)는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말합니다. 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행됩니다. 배치 정규화를 요약하면 다음과 같습니다. 입력에 대해 평균을 0으로 만들고, 정규화를 합니다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행합니다. 이 때 두 개의 매개변수 $\\gamma$ 와 $\\beta$ 를 사용하는데, $\\gamma$ 는 스케일을 위해 사용하고, $\\beta$ 는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 합니다.\n",
    "\n",
    "배치 정규화의 수식은 다음과 같습니다. 아래에서 $BN$은 배치 정규화를 의미합니다.\n",
    "\n",
    "  - $\\operatorname{Input}$ : 미니 배치 $B=\\{ x^{(1)},\\,x^{(2)},\\ldots,\\,x^{(m)}\\}$\n",
    "  - $\\operatorname{Output}$ : $y^{(i)}=BN_{\\gamma,\\,\\beta} (x(i))$\n",
    "\n",
    "$\\mu_B \\longleftarrow \\dfrac{1}{m}\\sum_i x^{(i)}$  # 미니 배치에 대한 평균\n",
    "\n",
    "$\\sigma^2_B  \\longleftarrow \\dfrac{1}{m} \\sum_i \\left( x^{(i)} - \\mu_B \\right)^2$  # 미니 배치에 대한 분산\n",
    "\n",
    "$\\hat{x}^{(i)} \\longleftarrow \\dfrac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2+\\varepsilon}}$  # 정규화\n",
    "\n",
    "$y^{(i)} \\longleftarrow  \\gamma \\hat{x}^{(i)} + \\beta = BN_{\\gamma,\\,\\beta} \\left( x^{(i)} \\right)$ # 스케일 조정과 시프트\n",
    "\n",
    "   - $m$ 은 미니 배치에 있는 샘플의 수\n",
    "   - $\\mu_B$ 는 미니 배치 $B$ 에 대한 평균.\n",
    "   - $\\sigma_B$ 는 미니 배치 $B$에 대한 표준편차.\n",
    "   - $\\hat{x}^{(i)}$ 은 평균이 0이고 정규화 된 입력 데이터.\n",
    "   - $\\varepsilon$ 은 분모가 0이 되는 것을 막는 작은 수. 보편적으로 $10^−5$.\n",
    "   - $\\gamma$ 는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상\n",
    "   - $\\beta$ 는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상\n",
    "   - $y^{(i)}$ 는 스케일과 시프트를 통해 조정한 $BN$의 최종 결과\n",
    "   \n",
    "   \n",
    "- 배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 합니다.\n",
    "\n",
    "   - 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선됩니다.\n",
    "   - 가중치 초기화에 훨씬 덜 민감해집니다.\n",
    "   - 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킵니다.\n",
    "   - 미니 배치마다 평균과 표준편차를 계산하므로 훈련 데이터에 일종의 잡음을 넣는 부수 효과로 과적합을 방지하는 효과도 냅니다. 하지만 부수적 효과이므로 드롭 아웃과 함께 사용하는 것이 좋습니다.\n",
    "   - 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려집니다. 그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요합니다.\n",
    "   - 배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있습니다. : https://arxiv.org/pdf/1805.11604.pdf\n",
    "\n",
    "\n",
    "<b>3) 배치 정규화의 한계 </b>\n",
    "배치 정규화는 뛰어난 방법이지만 몇 가지 한계가 존재합니다.\n",
    "\n",
    "- 1. 미니 배치 크기에 의존적이다.\n",
    "배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있습니다. 단적으로 배치 크기를 1로 하게되면 분산은 0이 됩니다. 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있습니다. 배치 정규화를 적용할때는 작은 미니 배치보다는 크기가 어느정도 되는 미니 배치에서 하는 것이 좋습니다. 이처럼 배치 정규화는 배치 크기에 의존적인 면이 있습니다.\n",
    "\n",
    "- 2. RNN에 적용하기 어렵다.\n",
    "뒤에서 배우겠지만, RNN은 각 시점(time step)마다 다른 통계치를 가집니다. 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만듭니다. RNN에서 배치 정규화를 적용하기 위한 몇 가지 논문이 제시되어 있지만, 여기서는 이를 소개하는 대신 배치 크기에도 의존적이지 않으며, RNN에도 적용하는 것이 수월한 층 정규화(layer normalization)라는 방법을 소개하고자 합니다.\n",
    "\n",
    "## 5. 층 정규화(Layer Normalization)\n",
    "층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보겠습니다. 다음은 m이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여줍니다. 미니 배치란 동일한 특성(feature) 개수들을 가진 다수의 샘플들을 의미함을 상기합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92938c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
