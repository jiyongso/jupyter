{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stylish-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "#import ipywidgets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection    \n",
    " \n",
    "# use LaTeX, choose nice some looking fonts and tweak some settings\n",
    "matplotlib.rc('font', family='serif')\n",
    "matplotlib.rc('font', size=16)\n",
    "matplotlib.rc('legend', fontsize=16)\n",
    "matplotlib.rc('legend', numpoints=1)\n",
    "matplotlib.rc('legend', handlelength=1.5)\n",
    "matplotlib.rc('legend', frameon=True)\n",
    "matplotlib.rc('xtick.major', pad=7)\n",
    "matplotlib.rc('xtick', direction=\"in\")\n",
    "matplotlib.rc('ytick', direction=\"in\")\n",
    "matplotlib.rc('xtick', top = True)\n",
    "matplotlib.rc('ytick', right =True )\n",
    "matplotlib.rc('xtick.minor', pad=7)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "# matplotlib.rc('text.latex', \n",
    "#               preamble=[r'\\usepackage[T1]{fontenc}',\n",
    "#                         r'\\usepackage{amsmath}',\n",
    "#                         r'\\usepackage{txfonts}',\n",
    "#                         r'\\usepackage{textcomp}'])\n",
    "\n",
    "matplotlib.rc('figure', figsize=(12, 9))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-rocket",
   "metadata": {},
   "source": [
    "V. 소프트맥스 회귀\n",
    "===\n",
    "\n",
    "이번 챕터에서는 3개 이상의 선택지로부터 1개를 선택하는 문제인 다중 클래스 분류(Multi-Class classification)를 풀기 위한 소프트맥스 회귀에 대해서 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-slave",
   "metadata": {},
   "source": [
    "## 4.1 원 핫 인코딩 (One-hot encoding)\n",
    "**원-핫 인코딩**은 선택해야 하는 선택지의 개수만큼의 차원을 가지면서, 각 선택지의 인덱스에 해당하는 원소에는 1, 나머지 원소는 0의 값을 가지도록 하는 표현 방법. 원-핫 인코딩으로 표현된 벡터를 **원-핫 벡터**(one-hot vector)라 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-portable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "regulated-royal",
   "metadata": {},
   "source": [
    "## 4.2 소프트맥스 회귀(Softmax Regression) 이해하기\n",
    "앞서 로지스틱 회귀를 통해 2개의 선택지 중에서 1개를 고르는 이진 분류(Binary Classification)를 풀어봤습니다. 이번 챕터에서는 소프트맥스 회귀를 통해 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류(Multi-Class Classification)를 실습해봅시다.\n",
    "\n",
    "#### 소프트맥스 함수\n",
    "$N$ 차원 벡터에서 $i$번째 원소가 $z_i$ 이며 $i$번째 클라스가 정답인 확률이 $p_i$ 일 때, 소프트맥스 함수는 $p_i$ 를 다음과 같이 정의한다.\n",
    "$$\n",
    "p_i = \\dfrac{e^{z_i}}{\\displaystyle\\sum_{j=1}^N e^{z_i}}\n",
    "$$\n",
    "\n",
    "#### 소프트맥스 비용 함수\n",
    "\n",
    "$N$ 개의 클라스로 분류하는 소프트맥스 회귀에서는 비용함수로 다음과 같이 정의된 크로스엔트로피 함수를 사용한다. $y_j$ 를 실제값 원-핫 벡터의 $j$ 번째 인덱스이고, $p_j$ 가 $j$ 번째 클래스일 확률 일 때,\n",
    "$$\n",
    "\\text{cost}(W) = -\\dfrac{1}{N}\\sum_{j=1}^N y_j \\log p_j\n",
    "$$\n",
    "이다.$N=2$ 일 때 위의 비용함수는 Logistic 회귀에서의 비용함수와 같음을 쉽게 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "public-texas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis= tensor([[0.3412, 0.2938, 0.2671, 0.4002, 0.2469],\n",
      "        [0.3559, 0.3306, 0.3796, 0.3393, 0.3719],\n",
      "        [0.3029, 0.3756, 0.3533, 0.2605, 0.3812]], grad_fn=<SoftmaxBackward>)\n",
      "y =  tensor([0, 2, 1])\n",
      "cost =  tensor(1.0078, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def softmax1(vervose=False):\n",
    "    torch.manual_seed(1)\n",
    "    # 입력값\n",
    "    z = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "\n",
    "    hypothesis = F.softmax(z, dim=0)\n",
    "    if vervose :\n",
    "        print(\"hypothesis=\", hypothesis)\n",
    "    y = torch.randint(5, (3,)).long()\n",
    "    if vervose :\n",
    "        print(\"y = \", y)\n",
    "    # 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "    y_one_hot = torch.zeros_like(hypothesis) \n",
    "    y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "#     print(y.unsqueeze(1))\n",
    "#     print(y_one_hot)\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "    print(\"cost = \", cost)\n",
    "softmax1(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-consumption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stupid-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experienced-lease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weighted-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random, time\n",
    "\n",
    "def mnist(pu=0):\n",
    "    t1=time.time()\n",
    "    ddv = ('cpu', 'cuda:0', 'cuda:1')\n",
    "    device = torch.device(ddv[pu])\n",
    "    if pu in (1, 2):\n",
    "        torch.cuda.manual_seed_all(777)\n",
    "    else :\n",
    "         torch.manual_seed(1)\n",
    "        \n",
    "    # hyperparameters\n",
    "    training_epochs = 15\n",
    "    batch_size = 100\n",
    "\n",
    "    mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "    mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "\n",
    "    data_loader = DataLoader(dataset=mnist_train,\n",
    "                                              batch_size=batch_size, # 배치 크기는 100\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    \n",
    "    # MNIST data image of shape 28 * 28 = 784\n",
    "    linear = nn.Linear(784, 10, bias=True).to(device)\n",
    "    \n",
    "    \n",
    "    # 비용 함수와 옵티마이저 정의\n",
    "    criterion = nn.CrossEntropyLoss().to(device) # 내부적으로 소프트맥스 함수를 포함하고 있음.\n",
    "    optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.\n",
    "        avg_cost = 0\n",
    "        total_batch = len(data_loader)\n",
    "\n",
    "        for X, Y in data_loader:\n",
    "            # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            hypothesis = linear(X)\n",
    "            cost = criterion(hypothesis, Y)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += cost / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    t2=time.time()\n",
    "    print('Learning finished for '+str(t2-t1)+\" sec\")\n",
    "\n",
    "    # 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "    with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "        X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "        Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "        prediction = linear(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print('Accuracy:', accuracy.item())\n",
    "\n",
    "        # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "        r = random.randint(0, len(mnist_test) - 1)\n",
    "        X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "        Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "        print('Label: ', Y_single_data.item())\n",
    "        single_prediction = linear(X_single_data)\n",
    "        print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "        plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "        plt.show()\n",
    "    print('testing finished for '+str(time.time()-t2)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-brush",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
