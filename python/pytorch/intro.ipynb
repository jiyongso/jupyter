{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "#import ipywidgets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection    \n",
    " \n",
    "# use LaTeX, choose nice some looking fonts and tweak some settings\n",
    "matplotlib.rc('font', family='serif')\n",
    "matplotlib.rc('font', size=16)\n",
    "matplotlib.rc('legend', fontsize=16)\n",
    "matplotlib.rc('legend', numpoints=1)\n",
    "matplotlib.rc('legend', handlelength=1.5)\n",
    "matplotlib.rc('legend', frameon=False)\n",
    "matplotlib.rc('xtick.major', pad=7)\n",
    "matplotlib.rc('xtick.minor', pad=7)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "# matplotlib.rc('text.latex', \n",
    "#               preamble=[r'\\usepackage[T1]{fontenc}',\n",
    "#                         r'\\usepackage{amsmath}',\n",
    "#                         r'\\usepackage{txfonts}',\n",
    "#                         r'\\usepackage{textcomp}'])\n",
    "\n",
    "matplotlib.rc('figure', figsize=(12, 9))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "extended-lodging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor.clamp 함수 = np.array.clip  함수\n",
    "xx = torch.tensor([[1,2,3,4], [2,3,4,5], [3,4,5,6]], dtype=torch.float)\n",
    "x2=xx.clamp(min=2.0, max=4.0)\n",
    "\n",
    "yy = np.array(xx)\n",
    "y2=yy.clip(min = 2.0, max=4.0)\n",
    "\n",
    "(np.array(x2)==y2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-observation",
   "metadata": {},
   "source": [
    "## Numpy를 통해 구현한 순전파와 역전파.\n",
    "\n",
    "- $x$ : 입력값\n",
    "- $y$ : 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39783166.91245361\n",
      "100 980.0078790535046\n",
      "200 9.130226112970881\n",
      "300 0.1282972340540697\n",
      "400 0.002151284614535909\n",
      "500 4.034382334978946e-05\n",
      "600 8.152841189633685e-07\n",
      "700 1.7316900406064005e-08\n",
      "800 3.802098951774234e-10\n",
      "900 8.534367441378677e-12\n"
     ]
    }
   ],
   "source": [
    "# N :배치 크기\n",
    "# D_in : 입력 차원\n",
    "# H : 은닉층의 차원\n",
    "# D_out : 출력 차원\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "# np.random.randn : 표준정규분포를 따르는 난수 발생.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t%100 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "grateful-surveillance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35434952.0\n",
      "100 422.3948974609375\n",
      "200 10.741645812988281\n",
      "300 7.255825996398926\n",
      "400 5.640827655792236\n",
      "500 9.950870513916016\n",
      "600 5.928495407104492\n",
      "700 8.997459411621094\n",
      "800 9.814862251281738\n",
      "900 5.415840148925781\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았습니다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1000):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\n",
    "    # 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도\n",
    "    # 되므로 중간값들에 대한 참조(reference)를 갖고 있을 필요가 없습니다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
    "    # 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # 다른 방법은 weight.data 및 weight.grad.data를 조작하는 방법입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을\n",
    "    # 추적하지 않는다는 것을 기억하십시오.\n",
    "    # 또한, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "imported-belize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([3.], grad_fn=<AddBackward0>)\n",
      "===== Run backward =====\n",
      "tensor([3.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(\"===== Run backward =====\")\n",
    "y.backward()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "senior-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "y= tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "z= tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "out= tensor(27., grad_fn=<MeanBackward0>)\n",
      "===== out.backward ===\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(\"x=\", x)\n",
    "\n",
    "y = x + 2\n",
    "print(\"y=\", y)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(\"z=\", z)\n",
    "print(\"out=\", out)\n",
    "\n",
    "print(\"===== out.backward ===\")\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-analysis",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{out}=\\overline{z}=\\dfrac{1}{4}\\sum_{i,\\,j=1}^2 z_{ij} =\\dfrac{1}{4}\\sum_{i,\\,j=1}^2 3{y_{ij}}^2 = \\dfrac{1}{4}\\sum_{i,\\,j=1}^2 3(x_{ij}+2)^2 \\\\\n",
    "\\dfrac{\\partial }{\\partial x} \\text{out}= \\sum \\dfrac{3}{2}(x+2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-honey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
